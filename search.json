[
  {
    "objectID": "transformers.mha.html",
    "href": "transformers.mha.html",
    "title": "Multi-Headed Attention (MHA) Transformer",
    "section": "",
    "text": "source\n\nPrepareForMHA\n\n PrepareForMHA (d_model:int, heads:int, d_k:int, bias:bool)\n\nPrepare for multi-head attention.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nd_model\nint\nNumber of features in the query, key and value vectors.\n\n\nheads\nint\nNumber of heads\n\n\nd_k\nint\nNumber of dimensions in vectors on each head\n\n\nbias\nbool\n\n\n\n\n\n\nExported source\nclass PrepareForMHA(nn.Module):\n    \"Prepare for multi-head attention.\"\n\n    def __init__(self, \n                 d_model: int, # Number of features in the `query`, `key` and `value` vectors.\n                 heads: int, # Number of heads\n                 d_k: int, # Number of dimensions in vectors on each head\n                 bias: bool\n                ):\n        \n        super().__init__()\n        # Linear layer for linear transform\n        self.linear = nn.Linear(d_model, heads * d_k, bias=bias)\n        \n        self.heads = heads\n        self.d_k = d_k\n\n\nThe PrepareForHMA module performs a linear transformation and splits the input vector into given number of heads for multi-head attention. This is used to transform key, query, and value vectors.\n\nsource\n\n\nPrepareForMHA.forward\n\n PrepareForMHA.forward (x:torch.Tensor)\n\nForward pass.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nx\nTensor\nInput has shape [seq_len, batch_size, d_model] or [batch_size, d_model]\n\n\nReturns\nTensor\nOutput has shape [seq_len, batch_size, heads, d_k] or [batch_size, heads, d_model]\n\n\n\n\n\nExported source\n@patch\ndef forward(self:PrepareForMHA, \n            x: torch.Tensor # Input has shape `[seq_len, batch_size, d_model]` or `[batch_size, d_model]`\n            ) -&gt; torch.Tensor: # Output has shape `[seq_len, batch_size, heads, d_k]` or `[batch_size, heads, d_model]`\n    \"Forward pass.\"\n    \n    # We apply the linear transformation to the last dimension and split that into the heads.\n    head_shape = x.shape[:-1]\n    # Linear transform\n    x = self.linear(x)\n    # Split last dimension into heads\n    x = x.view(*head_shape, self.heads, self.d_k)\n\n    return x\n\n\nThe forward pass of PrepareForHMA applies the linear transformation to the last dimension and split that into the heads.\n\nsource\n\n\nMultiHeadAttention\n\n MultiHeadAttention (heads:int, d_model:int, dropout_prob:float=0.1,\n                     bias:bool=True)\n\nA Multi-Head Attention Module.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nheads\nint\n\nNumber of heads\n\n\nd_model\nint\n\nNumber of features in the query, key and value vectors.\n\n\ndropout_prob\nfloat\n0.1\n\n\n\nbias\nbool\nTrue\nWhether to have a bias parameter for transformations for Q, K and V\n\n\n\n\n\nExported source\nclass MultiHeadAttention(nn.Module):\n    \"A Multi-Head Attention Module.\"\n\n    def __init__(self, \n                 heads: int, # Number of heads\n                 d_model: int, # Number of features in the `query`, `key` and `value` vectors.\n                 dropout_prob: float = 0.1, \n                 bias: bool = True # Whether to have a bias parameter for transformations for Q, K and V\n                ):\n\n        super().__init__()\n\n        # Number of features per head\n        self.d_k = d_model // heads\n        # Number of heads\n        self.heads = heads\n\n        # These transform the `query`, `key` and `value` vectors for multi-headed attention.\n        self.query = PrepareForMHA(d_model, heads, self.d_k, bias=bias)\n        self.key = PrepareForMHA(d_model, heads, self.d_k, bias=bias)\n        self.value = PrepareForMHA(d_model, heads, self.d_k, bias=True)\n\n        # Softmax for attention along the time dimension of `key`\n        self.softmax = nn.Softmax(dim=1)\n\n        # Output layer\n        self.output = nn.Linear(d_model, d_model)\n        # Dropout\n        self.dropout = nn.Dropout(dropout_prob)\n        # Scaling factor before the softmax\n        self.scale = 1 / math.sqrt(self.d_k)\n\n        # We store attentions so that it can be used for logging, or other computations if needed\n        self.attn = None\n\n\n\nsource\n\n\nMultiHeadAttention.get_scores\n\n MultiHeadAttention.get_scores (query:torch.Tensor, key:torch.Tensor)\n\nCalculate scores between queries and keys.\n\n\n\n\nType\nDetails\n\n\n\n\nquery\nTensor\nInput query\n\n\nkey\nTensor\nInput key\n\n\nReturns\nTensor\n\n\n\n\nMultiHeadAttention.get_scores() computes \\(Q K^\\top\\) or \\(S_{ijbh} = \\sum_d Q_{ibhd} K_{jbhd}\\). This method can be overridden for other variations like relative attention.\n\nsource\n\n\nMultiHeadAttention.prepare_mask\n\n MultiHeadAttention.prepare_mask (mask:torch.Tensor,\n                                  query_shape:List[int],\n                                  key_shape:List[int])\n\nPrepare mask for MHA.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nmask\nTensor\nIt has shape [seq_len_q, seq_len_k, batch_size], where first dimension is the query dimension.\n\n\nquery_shape\nList\n\n\n\nkey_shape\nList\n\n\n\nReturns\nTensor\nResulting mask has shape [seq_len_q, seq_len_k, batch_size, heads]\n\n\n\n\nsource\n\n\nMultiHeadAttention.forward\n\n MultiHeadAttention.forward (query:torch.Tensor, key:torch.Tensor,\n                             value:torch.Tensor,\n                             mask:Optional[torch.Tensor]=None)\n\nForward pass.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nquery\nTensor\n\nTensor that stores collection of query vectors\n\n\nkey\nTensor\n\nTensor that stores collection of key vectors\n\n\nvalue\nTensor\n\nTensor that stores collection of value vectors\n\n\nmask\nOptional\nNone\n\n\n\n\n\n\nExported source\n@patch\ndef forward(self:MultiHeadAttention,\n            query: torch.Tensor, # Tensor that stores collection of *query* vectors\n            key: torch.Tensor, # Tensor that stores collection of *key* vectors\n            value: torch.Tensor, # Tensor that stores collection of *value* vectors\n            mask: Optional[torch.Tensor] = None\n            ):\n    \"Forward pass.\"\n\n    # `query`, `key` and `value`  have shape `[seq_len, batch_size, d_model]`\n    seq_len, batch_size, _ = query.shape\n\n    if mask is not None:\n        mask = self.prepare_mask(mask, query.shape, key.shape)\n\n    # Prepare `query`, `key` and `value` for attention computation.\n    # These will then have shape `[seq_len, batch_size, heads, d_k]`.\n    query = self.query(query)\n    key = self.key(key)\n    value = self.value(value)\n\n1    scores = self.get_scores(query, key)\n2    scores *= self.scale\n    if mask is not None:\n3        scores = scores.masked_fill(mask == 0, float('-inf'))\n\n4    attn = self.softmax(scores)\n\n    # Save attentions if debugging\n    tracker.debug('attn', attn)\n\n    # Apply dropout\n    attn = self.dropout(attn)\n\n5    x = torch.einsum(\"ijbh,jbhd-&gt;ibhd\", attn, value)\n\n    # Save attentions for any other calculations \n    self.attn = attn.detach()\n\n    # Concatenate multiple heads\n    x = x.reshape(seq_len, batch_size, -1)\n\n    # Output layer\n    return self.output(x)\n\n\n\n1\n\nCompute attention scores \\(Q K^\\top\\). This gives a tensor of shape [seq_len, seq_len, batch_size, heads]\n\n2\n\nScale scores \\(\\frac{Q K^\\top}{\\sqrt{d_k}}\\)\n\n3\n\nApply mask\n\n4\n\n\\(softmax\\) attention along the key sequence dimension \\(\\underset{seq}{softmax}\\Bigg(\\frac{Q K^\\top}{\\sqrt{d_k}}\\Bigg)\\)\n\n5\n\nMultiply by values \\(\\underset{seq}{softmax}\\Bigg(\\frac{Q K^\\top}{\\sqrt{d_k}}\\Bigg)V\\)\n\n\n\n\nThe forward pass of MultiHeadAttention computes scaled multi-headed attention for given query, key and value vectors. query, key and value are the tensors that store collection of query, key and value vectors. They have shape [seq_len, batch_size, d_model], mask has shape [seq_len, seq_len, batch_size] and mask[i, j, b] indicates whether for batch b, query at position i has access to key-value at position j.\n\\[\\mathop{Attention}(Q, K, V) = \\underset{seq}{\\mathop{softmax}}\\Bigg(\\frac{Q K^\\top}{\\sqrt{d_k}}\\Bigg)V\\]\nIn simple terms, it finds keys that matches the query, and gets the values of those keys. It uses dot-product of query and key as the indicator of how matching they are. Before taking the \\(softmax\\) the dot-products are scaled by \\(\\frac{1}{\\sqrt{d_k}}\\). This is done to avoid large dot-product values causing softmax to give very small gradients when \\(d_k\\) is large. Softmax is calculated along the axis of of the sequence (or time).",
    "crumbs": [
      "Transformers",
      "Multi-Headed Attention (MHA) Transformer"
    ]
  },
  {
    "objectID": "transformers.relative_mha.html",
    "href": "transformers.relative_mha.html",
    "title": "Relative Multi-Headed Attention",
    "section": "",
    "text": "source\n\nshift_right\n\n shift_right (x:torch.Tensor)\n\nThis method shifts the \\(i^{th}\\) row of a matrix by \\(i\\) columns.\nFor example, if the input is [[1, 2 ,3], [4, 5 ,6], [7, 8, 9]], the shifted result would be [[1, 2 ,3], [0, 4, 5], [6, 0, 7]].\n\na = torch.tensor([[1, 2 ,3], [4, 5 ,6], [7, 8, 9]])\nb = torch.tensor([[1, 2 ,3], [0, 4, 5], [6, 0, 7]])\ntest_eq(shift_right(a), b)\n\nWe can visualize tensors with inspect():\n\nc = torch.arange(1, 6)[None, :, None, None].repeat(5, 1, 1, 1)\ninspect(c[:, :, 0, 0])\n\n[\n [1, 2, 3, 4, 5 ], \n [1, 2, 3, 4, 5 ], \n [1, 2, 3, 4, 5 ], \n [1, 2, 3, 4, 5 ], \n [1, 2, 3, 4, 5 ]\n]\n\n\n\ninspect(shift_right(c)[:, :, 0, 0])\n\n[\n [1, 2, 3, 4, 5 ], \n [0, 1, 2, 3, 4 ], \n [5, 0, 1, 2, 3 ], \n [4, 5, 0, 1, 2 ], \n [3, 4, 5, 0, 1 ]\n]\n\n\n\nsource\n\n\nRelativeMultiHeadAttention\n\n RelativeMultiHeadAttention (heads:int, d_model:int,\n                             dropout_prob:float=0.1)\n\nRelative Multi-Head Attention Module.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nheads\nint\n\n\n\n\nd_model\nint\n\n\n\n\ndropout_prob\nfloat\n0.1\n&lt;1&gt;\n\n\n\n\n\nExported source\nclass RelativeMultiHeadAttention(MultiHeadAttention):\n    \"Relative Multi-Head Attention Module.\"\n\n1    def __init__(self, heads: int, d_model: int, dropout_prob: float = 0.1):\n        \n2        super().__init__(heads, d_model, dropout_prob, bias=False)\n\n        self.P = 2 ** 12\n3        self.key_pos_embeddings = nn.Parameter(torch.zeros((self.P * 2, heads, self.d_k)), requires_grad=True)\n4        self.key_pos_bias = nn.Parameter(torch.zeros((self.P * 2, heads)), requires_grad=True)\n5        self.query_pos_bias = nn.Parameter(torch.zeros((heads, self.d_k)), requires_grad=True)\n\n\n\n1\n\nThe linear transformations do not need a bias since we explicitly include it when calculating scores. However having a bias for value might make sense.\n\n2\n\nNumber of relative positions\n\n3\n\nRelative positional embeddings for key relative to the query. We need \\(2P\\) embeddings because the keys can be before or after the query\n\n4\n\nRelative positional embedding bias for key relative to the query.\n\n5\n\nPositional embeddings for the query is independent of the position of the query\n\n\n\n\n\nsource\n\n\nRelativeMultiHeadAttention.get_scores\n\n RelativeMultiHeadAttention.get_scores (query:torch.Tensor,\n                                        key:torch.Tensor)\n\nGet relative attention scores.\n\n\n\n\nType\nDetails\n\n\n\n\nquery\nTensor\n\n\n\nkey\nTensor\n&lt;1&gt;\n\n\n\n\n\nExported source\n@patch\n1def get_scores(self: RelativeMultiHeadAttention, query:torch.Tensor, key:torch.Tensor):\n    \"Get relative attention scores.\"\n\n2    key_pos_emb = self.key_pos_embeddings[self.P - key.shape[0]:self.P + query.shape[0]]\n3    key_pos_bias = self.key_pos_bias[self.P - key.shape[0]:self.P + query.shape[0]]\n4    query_pos_bias = self.query_pos_bias[None, None, :, :]\n5    ac = torch.einsum('ibhd,jbhd-&gt;ijbh', query + query_pos_bias, key)\n6    b = torch.einsum('ibhd,jhd-&gt;ijbh', query, key_pos_emb)\n7    d = key_pos_bias[None, :, None, :]\n8    bd = shift_right(b + d)\n9    bd = bd[:, -key.shape[0]:]\n\n    # Return the sum\n    return ac + bd\n\n\n\n1\n\nWe override the MultiHeadAttention module so we only need to write the get_scores() method.\n\n2\n\n\\(R_k\\)\n\n3\n\n\\(S_k\\)\n\n4\n\n\\({v^\\top}\\)\n\n5\n\n\\(\\mathbf{A + C}_{i,j} = Q_i^\\top K_j + v^\\top K_j\\)\n\n6\n\n\\(\\mathbf{B'}_{i,k} = Q_i^\\top {R_k}\\)\n\n7\n\n\\(\\mathbf{D'}_{i,k} = {S_k}\\)\n\n8\n\nShift the rows of \\(\\mathbf{(B' + D')}_{i,k}\\) to get \\(\\mathbf{(B + D)}_{i,j} = \\mathbf{(B' + D')}_{i,i - j}\\)\n\n9\n\nRemove extra positions\n\n\n\n\nWith absolute attention \\(A^{abs}_{j} = lin_q(X^q_i + P_i)^\\top lin_k(X^k_j + P_j) = {Q_i^\\top K_j} + {Q_i^\\top U^K_j} + {U^Q_i}^\\top K_j + {{U^Q_i}^\\top U^K_j}\\), where \\(Q_i, K_j\\), are linear transformations of original embeddings \\(X^q_i, X^k_j\\) and \\(U^Q_i, U^K_j\\) are linear transformations of absolute positional encodings \\(P_i, P_j\\). They reason out that the attention to a given key should be the same regardless of the position of query. Hence replace \\({U^Q_i}^\\top K_j\\) with a constant \\({v^\\top} K_j\\).\nFor the second and third terms relative positional encodings are introduced. So \\({Q_i^\\top U^K_j}\\) is replaced with \\({Q_i^\\top {R_{i - j}}}\\) and \\({{U^Q_i}^\\top U^K_j}\\) with \\({{S_{i-j}}}\\), \\(A^{rel}_{i,j} = \\mathbf{Q_i^\\top K_j} + {Q_i^\\top {R_{i - j}}} + {{v^\\top} K_j} + {S_{i-j}}\\)",
    "crumbs": [
      "Transformers",
      "Relative Multi-Headed Attention"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "mlrpg",
    "section": "",
    "text": "This is a collection of PyTorch implementations -documented with explanations- of some key research papers on machine learning, deep learning and related algorithms and models, inspired by this website from @labmlai.\nThe main goal of this repository is to be a machine learning research playground where I can develop a hands-on understanding of these algorithms and models, and where I can practice my skills of translating research papers into working code.\nMy main “tools of the trade” are Python and PyTorch. My coding/development platform comprises VS Code, Jupyter Lab, nbdev (for literate programming) and Paperspace Gradient. I use nbdev and Quarto for publishing.",
    "crumbs": [
      "mlrpg"
    ]
  },
  {
    "objectID": "index.html#papers",
    "href": "index.html#papers",
    "title": "mlrpg",
    "section": "Papers",
    "text": "Papers\nThe list of papers and associated implementations include the following, in a somehow not very strict taxonomy:\n\n\nTransformers\n\n\nAttention-Free Transformer: An Attention Free Transformer &gt; Implementation\nRelative Attention Transformer: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context &gt; Implementation",
    "crumbs": [
      "mlrpg"
    ]
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "mlrpg",
    "section": "Install",
    "text": "Install\npip install mlrpg",
    "crumbs": [
      "mlrpg"
    ]
  }
]