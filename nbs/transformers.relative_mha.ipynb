{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp transformers.relative_mha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export\n",
    "\n",
    "from fastcore.basics import patch\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relative Multi-Headed Attention\n",
    "> Annotated [PyTorch](https://pytorch.org) implementation of Relative Multi-Headed Attention from the paper [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| hide\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from fastcore.test import *\n",
    "\n",
    "from labml.logger import inspect\n",
    "from mlrpg.transformers.mha import MultiHeadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def shift_right(x: torch.Tensor):\n",
    "    \"This method shifts the $i^{th}$ row of a matrix by $i$ columns.\"\n",
    "\n",
    "    #*Ideally we should mask out the lower triangle but it's ok for our purpose*.\n",
    "\n",
    "    # Concatenate a column of zeros\n",
    "    zero_pad = x.new_zeros(x.shape[0], 1, *x.shape[2:])\n",
    "    x_padded = torch.cat([x, zero_pad], dim=1)\n",
    "\n",
    "    # Reshape and remove excess elements from the end\n",
    "    x_padded = x_padded.view(x.shape[1] + 1, x.shape[0], *x.shape[2:])\n",
    "    x = x_padded[:-1].view_as(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if the input is `[[1, 2 ,3], [4, 5 ,6], [7, 8, 9]]`, the shifted result would be `[[1, 2 ,3], [0, 4, 5], [6, 0, 7]]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a = torch.tensor([[1, 2 ,3], [4, 5 ,6], [7, 8, 9]])\n",
    "b = torch.tensor([[1, 2 ,3], [0, 4, 5], [6, 0, 7]])\n",
    "test_eq(shift_right(a), b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize tensors with `inspect()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"overflow-x: scroll;\"><strong></strong><span style=\"color: #C5C1B4\">[</span><strong></strong>\n",
       "<strong></strong><strong> </strong><span style=\"color: #C5C1B4\">[</span><strong>1</strong><strong>, </strong><strong>2</strong><strong>, </strong><strong>3</strong><strong>, </strong><strong>4</strong><strong>, </strong><strong>5</strong><strong> </strong><span style=\"color: #C5C1B4\">]</span><strong>, </strong><strong></strong>\n",
       "<strong></strong><strong> </strong><span style=\"color: #C5C1B4\">[</span><strong>1</strong><strong>, </strong><strong>2</strong><strong>, </strong><strong>3</strong><strong>, </strong><strong>4</strong><strong>, </strong><strong>5</strong><strong> </strong><span style=\"color: #C5C1B4\">]</span><strong>, </strong><strong></strong>\n",
       "<strong></strong><strong> </strong><span style=\"color: #C5C1B4\">[</span><strong>1</strong><strong>, </strong><strong>2</strong><strong>, </strong><strong>3</strong><strong>, </strong><strong>4</strong><strong>, </strong><strong>5</strong><strong> </strong><span style=\"color: #C5C1B4\">]</span><strong>, </strong><strong></strong>\n",
       "<strong></strong><strong> </strong><span style=\"color: #C5C1B4\">[</span><strong>1</strong><strong>, </strong><strong>2</strong><strong>, </strong><strong>3</strong><strong>, </strong><strong>4</strong><strong>, </strong><strong>5</strong><strong> </strong><span style=\"color: #C5C1B4\">]</span><strong>, </strong><strong></strong>\n",
       "<strong></strong><strong> </strong><span style=\"color: #C5C1B4\">[</span><strong>1</strong><strong>, </strong><strong>2</strong><strong>, </strong><strong>3</strong><strong>, </strong><strong>4</strong><strong>, </strong><strong>5</strong><strong> </strong><span style=\"color: #C5C1B4\">]</span><strong></strong>\n",
       "<strong></strong><strong></strong><span style=\"color: #C5C1B4\">]</span></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "c = torch.arange(1, 6)[None, :, None, None].repeat(5, 1, 1, 1)\n",
    "inspect(c[:, :, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"overflow-x: scroll;\"><strong></strong><span style=\"color: #C5C1B4\">[</span><strong></strong>\n",
       "<strong></strong><strong> </strong><span style=\"color: #C5C1B4\">[</span><strong>1</strong><strong>, </strong><strong>2</strong><strong>, </strong><strong>3</strong><strong>, </strong><strong>4</strong><strong>, </strong><strong>5</strong><strong> </strong><span style=\"color: #C5C1B4\">]</span><strong>, </strong><strong></strong>\n",
       "<strong></strong><strong> </strong><span style=\"color: #C5C1B4\">[</span><strong>0</strong><strong>, </strong><strong>1</strong><strong>, </strong><strong>2</strong><strong>, </strong><strong>3</strong><strong>, </strong><strong>4</strong><strong> </strong><span style=\"color: #C5C1B4\">]</span><strong>, </strong><strong></strong>\n",
       "<strong></strong><strong> </strong><span style=\"color: #C5C1B4\">[</span><strong>5</strong><strong>, </strong><strong>0</strong><strong>, </strong><strong>1</strong><strong>, </strong><strong>2</strong><strong>, </strong><strong>3</strong><strong> </strong><span style=\"color: #C5C1B4\">]</span><strong>, </strong><strong></strong>\n",
       "<strong></strong><strong> </strong><span style=\"color: #C5C1B4\">[</span><strong>4</strong><strong>, </strong><strong>5</strong><strong>, </strong><strong>0</strong><strong>, </strong><strong>1</strong><strong>, </strong><strong>2</strong><strong> </strong><span style=\"color: #C5C1B4\">]</span><strong>, </strong><strong></strong>\n",
       "<strong></strong><strong> </strong><span style=\"color: #C5C1B4\">[</span><strong>3</strong><strong>, </strong><strong>4</strong><strong>, </strong><strong>5</strong><strong>, </strong><strong>0</strong><strong>, </strong><strong>1</strong><strong> </strong><span style=\"color: #C5C1B4\">]</span><strong></strong>\n",
       "<strong></strong><strong></strong><span style=\"color: #C5C1B4\">]</span></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inspect(shift_right(c)[:, :, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "\n",
    "class RelativeMultiHeadAttention(MultiHeadAttention):\n",
    "    \"Relative Multi-Head Attention Module.\"\n",
    "\n",
    "    def __init__(self, heads: int, d_model: int, dropout_prob: float = 0.1): # <1>\n",
    "        \n",
    "        super().__init__(heads, d_model, dropout_prob, bias=False) # <2>\n",
    "\n",
    "        self.P = 2 ** 12 # <1>\n",
    "        self.key_pos_embeddings = nn.Parameter(torch.zeros((self.P * 2, heads, self.d_k)), requires_grad=True) # <3>\n",
    "        self.key_pos_bias = nn.Parameter(torch.zeros((self.P * 2, heads)), requires_grad=True) # <4>\n",
    "        self.query_pos_bias = nn.Parameter(torch.zeros((heads, self.d_k)), requires_grad=True) # <5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The linear transformations do not need a bias since we explicitly include it when calculating scores. However having a bias for `value` might make sense.\n",
    "2. Number of relative positions\n",
    "3. Relative positional embeddings for key relative to the query. We need $2P$ embeddings because the keys can be before or after the query\n",
    "4. Relative positional embedding bias for key relative to the query.\n",
    "5. Positional embeddings for the query is independent of the position of the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "\n",
    "@patch\n",
    "def get_scores(self: RelativeMultiHeadAttention, query:torch.Tensor, key:torch.Tensor): # <1>\n",
    "    \"Get relative attention scores.\"\n",
    "\n",
    "    key_pos_emb = self.key_pos_embeddings[self.P - key.shape[0]:self.P + query.shape[0]] # <2>\n",
    "    key_pos_bias = self.key_pos_bias[self.P - key.shape[0]:self.P + query.shape[0]] # <3>\n",
    "    query_pos_bias = self.query_pos_bias[None, None, :, :] # <4>\n",
    "    ac = torch.einsum('ibhd,jbhd->ijbh', query + query_pos_bias, key) # <5>\n",
    "    b = torch.einsum('ibhd,jhd->ijbh', query, key_pos_emb) # <6>\n",
    "    d = key_pos_bias[None, :, None, :] # <7>\n",
    "    bd = shift_right(b + d) # <8>\n",
    "    bd = bd[:, -key.shape[0]:] # <9>\n",
    "\n",
    "    # Return the sum\n",
    "    return ac + bd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We override the `MultiHeadAttention` module so we only need to write the `get_scores()` method.\n",
    "2. $R_k$\n",
    "3. $S_k$\n",
    "4. ${v^\\top}$\n",
    "5. $\\mathbf{A + C}_{i,j} = Q_i^\\top K_j + v^\\top K_j$\n",
    "6. $\\mathbf{B'}_{i,k} = Q_i^\\top {R_k}$\n",
    "7. $\\mathbf{D'}_{i,k} = {S_k}$\n",
    "8. Shift the rows of $\\mathbf{(B' + D')}_{i,k}$ to get $\\mathbf{(B + D)}_{i,j} = \\mathbf{(B' + D')}_{i,i - j}$\n",
    "9. Remove extra positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With absolute attention  $A^{abs}_{j} = lin_q(X^q_i + P_i)^\\top lin_k(X^k_j + P_j) = {Q_i^\\top K_j} + {Q_i^\\top U^K_j} + {U^Q_i}^\\top K_j + {{U^Q_i}^\\top U^K_j}$, where $Q_i, K_j$, are linear transformations of original embeddings $X^q_i, X^k_j$ and $U^Q_i, U^K_j$ are linear transformations of absolute positional encodings $P_i, P_j$. They reason out that the attention to a given key should be the same regardless of the position of query. Hence replace ${U^Q_i}^\\top K_j$ with a constant ${v^\\top} K_j$.\n",
    "\n",
    "For the second and third terms relative positional encodings are introduced. So ${Q_i^\\top U^K_j}$ is replaced with ${Q_i^\\top {R_{i - j}}}$ and ${{U^Q_i}^\\top U^K_j}$ with ${{S_{i-j}}}$, $A^{rel}_{i,j} = \\mathbf{Q_i^\\top K_j} + {Q_i^\\top {R_{i - j}}} + {{v^\\top} K_j} + {S_{i-j}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
