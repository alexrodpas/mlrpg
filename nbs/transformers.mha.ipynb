{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp transformers.mha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#| hide\n",
    "#| export\n",
    "\n",
    "from fastcore.basics import patch\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Headed Attention (MHA) Transformer\n",
    "\n",
    "> Annotated [PyTorch](https://pytorch.org) implementation of the standard Multi-Headed Attention (MHA) Transformer architecture, introduced in the paper [\"Attention is All You Need\"](https://arxiv.org/abs/1706.03762).\n",
    "\n",
    "The implementation is inspired by [Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide \n",
    "#| export\n",
    "\n",
    "import math\n",
    "from typing import Optional, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from labml import tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "\n",
    "class PrepareForMHA(nn.Module):\n",
    "    \"Prepare for multi-head attention.\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 d_model: int, # Number of features in the `query`, `key` and `value` vectors.\n",
    "                 heads: int, # Number of heads\n",
    "                 d_k: int, # Number of dimensions in vectors on each head\n",
    "                 bias: bool\n",
    "                ):\n",
    "        \n",
    "        super().__init__()\n",
    "        # Linear layer for linear transform\n",
    "        self.linear = nn.Linear(d_model, heads * d_k, bias=bias)\n",
    "        \n",
    "        self.heads = heads\n",
    "        self.d_k = d_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `PrepareForHMA` module performs a linear transformation and splits the input vector into given number of heads for multi-head attention. This is used to transform **key**, **query**, and **value** vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "\n",
    "@patch\n",
    "def forward(self:PrepareForMHA, \n",
    "            x: torch.Tensor # Input has shape `[seq_len, batch_size, d_model]` or `[batch_size, d_model]`\n",
    "            ) -> torch.Tensor: # Output has shape `[seq_len, batch_size, heads, d_k]` or `[batch_size, heads, d_model]`\n",
    "    \"Forward pass.\"\n",
    "    \n",
    "    # We apply the linear transformation to the last dimension and split that into the heads.\n",
    "    head_shape = x.shape[:-1]\n",
    "    # Linear transform\n",
    "    x = self.linear(x)\n",
    "    # Split last dimension into heads\n",
    "    x = x.view(*head_shape, self.heads, self.d_k)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `forward` pass of `PrepareForHMA` applies the linear transformation to the last dimension and split that into the heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"A Multi-Head Attention Module.\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 heads: int, # Number of heads\n",
    "                 d_model: int, # Number of features in the `query`, `key` and `value` vectors.\n",
    "                 dropout_prob: float = 0.1, \n",
    "                 bias: bool = True # Whether to have a bias parameter for transformations for Q, K and V\n",
    "                ):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Number of features per head\n",
    "        self.d_k = d_model // heads\n",
    "        # Number of heads\n",
    "        self.heads = heads\n",
    "\n",
    "        # These transform the `query`, `key` and `value` vectors for multi-headed attention.\n",
    "        self.query = PrepareForMHA(d_model, heads, self.d_k, bias=bias)\n",
    "        self.key = PrepareForMHA(d_model, heads, self.d_k, bias=bias)\n",
    "        self.value = PrepareForMHA(d_model, heads, self.d_k, bias=True)\n",
    "\n",
    "        # Softmax for attention along the time dimension of `key`\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        # Output layer\n",
    "        self.output = nn.Linear(d_model, d_model)\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        # Scaling factor before the softmax\n",
    "        self.scale = 1 / math.sqrt(self.d_k)\n",
    "\n",
    "        # We store attentions so that it can be used for logging, or other computations if needed\n",
    "        self.attn = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@patch\n",
    "def get_scores(self:MultiHeadAttention, \n",
    "               query: torch.Tensor, # Input query\n",
    "               key: torch.Tensor # Input key\n",
    "               ) -> torch.Tensor:\n",
    "        \n",
    "        \"Calculate scores between queries and keys.\"\n",
    "\n",
    "        return torch.einsum('ibhd,jbhd->ijbh', query, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MultiHeadAttention.get_scores()` computes $Q K^\\top$ or $S_{ijbh} = \\sum_d Q_{ibhd} K_{jbhd}$. This method can be overridden for other variations like relative attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@patch\n",
    "def prepare_mask(self:MultiHeadAttention, \n",
    "                 mask: torch.Tensor, # It has shape `[seq_len_q, seq_len_k, batch_size]`, where first dimension is the query dimension.\n",
    "                 query_shape: List[int], \n",
    "                 key_shape: List[int]\n",
    "                ) -> torch.Tensor: # Resulting mask has shape `[seq_len_q, seq_len_k, batch_size, heads]`\n",
    "    \n",
    "    \"Prepare mask for MHA.\"\n",
    "    # If the query dimension is equal to $1$ it will be broadcasted.\n",
    "\n",
    "    assert mask.shape[0] == 1 or mask.shape[0] == query_shape[0]\n",
    "    assert mask.shape[1] == key_shape[0]\n",
    "    assert mask.shape[2] == 1 or mask.shape[2] == query_shape[1]\n",
    "\n",
    "    # Same mask applied to all heads.\n",
    "    mask = mask.unsqueeze(-1)\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "\n",
    "@patch\n",
    "def forward(self:MultiHeadAttention,\n",
    "            query: torch.Tensor, # Tensor that stores collection of *query* vectors\n",
    "            key: torch.Tensor, # Tensor that stores collection of *key* vectors\n",
    "            value: torch.Tensor, # Tensor that stores collection of *value* vectors\n",
    "            mask: Optional[torch.Tensor] = None\n",
    "            ):\n",
    "    \"Forward pass.\"\n",
    "\n",
    "    # `query`, `key` and `value`  have shape `[seq_len, batch_size, d_model]`\n",
    "    seq_len, batch_size, _ = query.shape\n",
    "\n",
    "    if mask is not None:\n",
    "        mask = self.prepare_mask(mask, query.shape, key.shape)\n",
    "\n",
    "    # Prepare `query`, `key` and `value` for attention computation.\n",
    "    # These will then have shape `[seq_len, batch_size, heads, d_k]`.\n",
    "    query = self.query(query)\n",
    "    key = self.key(key)\n",
    "    value = self.value(value)\n",
    "\n",
    "    scores = self.get_scores(query, key) # <1>\n",
    "    scores *= self.scale # <2>\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf')) # <3>\n",
    "\n",
    "    attn = self.softmax(scores) # <4>\n",
    "\n",
    "    # Save attentions if debugging\n",
    "    tracker.debug('attn', attn)\n",
    "\n",
    "    # Apply dropout\n",
    "    attn = self.dropout(attn)\n",
    "\n",
    "    x = torch.einsum(\"ijbh,jbhd->ibhd\", attn, value) # <5>\n",
    "\n",
    "    # Save attentions for any other calculations \n",
    "    self.attn = attn.detach()\n",
    "\n",
    "    # Concatenate multiple heads\n",
    "    x = x.reshape(seq_len, batch_size, -1)\n",
    "\n",
    "    # Output layer\n",
    "    return self.output(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Compute attention scores $Q K^\\top$. This gives a tensor of shape `[seq_len, seq_len, batch_size, heads]`\n",
    "2. Scale scores $\\frac{Q K^\\top}{\\sqrt{d_k}}$\n",
    "3. Apply mask\n",
    "4. $softmax$ attention along the key sequence dimension $\\underset{seq}{softmax}\\Bigg(\\frac{Q K^\\top}{\\sqrt{d_k}}\\Bigg)$\n",
    "5. Multiply by values $\\underset{seq}{softmax}\\Bigg(\\frac{Q K^\\top}{\\sqrt{d_k}}\\Bigg)V$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `forward` pass of `MultiHeadAttention` computes scaled multi-headed attention for given `query`, `key` and `value` vectors. `query`, `key` and `value` are the tensors that store collection of *query*, *key* and *value* vectors. They have shape `[seq_len, batch_size, d_model]`, `mask` has shape `[seq_len, seq_len, batch_size]` and `mask[i, j, b]` indicates whether for batch `b`, query at position `i` has access to key-value at position `j`.\n",
    "\n",
    "$$\\mathop{Attention}(Q, K, V) = \\underset{seq}{\\mathop{softmax}}\\Bigg(\\frac{Q K^\\top}{\\sqrt{d_k}}\\Bigg)V$$\n",
    "\n",
    "In simple terms, it finds keys that matches the query, and gets the values of those keys. It uses dot-product of query and key as the indicator of how matching they are. Before taking the $softmax$ the dot-products are scaled by $\\frac{1}{\\sqrt{d_k}}$. This is done to avoid large dot-product values causing softmax to give very small gradients when $d_k$ is large. Softmax is calculated along the axis of of the sequence (or time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
