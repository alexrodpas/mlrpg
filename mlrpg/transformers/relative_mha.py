# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/transformers.relative_mha.ipynb.

# %% auto 0
__all__ = ['shift_right', 'RelativeMultiHeadAttention']

# %% ../../nbs/transformers.relative_mha.ipynb 2
from fastcore.basics import patch
from nbdev.showdoc import *

# %% ../../nbs/transformers.relative_mha.ipynb 4
import torch
from torch import nn

from fastcore.test import *

from labml.logger import inspect
from .mha import MultiHeadAttention

# %% ../../nbs/transformers.relative_mha.ipynb 5
def shift_right(x: torch.Tensor):
    "This method shifts the $i^{th}$ row of a matrix by $i$ columns."

    #*Ideally we should mask out the lower triangle but it's ok for our purpose*.

    # Concatenate a column of zeros
    zero_pad = x.new_zeros(x.shape[0], 1, *x.shape[2:])
    x_padded = torch.cat([x, zero_pad], dim=1)

    # Reshape and remove excess elements from the end
    x_padded = x_padded.view(x.shape[1] + 1, x.shape[0], *x.shape[2:])
    x = x_padded[:-1].view_as(x)

    return x

# %% ../../nbs/transformers.relative_mha.ipynb 11
class RelativeMultiHeadAttention(MultiHeadAttention):
    "Relative Multi-Head Attention Module."

    def __init__(self, heads: int, d_model: int, dropout_prob: float = 0.1): # <1>
        
        super().__init__(heads, d_model, dropout_prob, bias=False) # <2>

        self.P = 2 ** 12 # <1>
        self.key_pos_embeddings = nn.Parameter(torch.zeros((self.P * 2, heads, self.d_k)), requires_grad=True) # <3>
        self.key_pos_bias = nn.Parameter(torch.zeros((self.P * 2, heads)), requires_grad=True) # <4>
        self.query_pos_bias = nn.Parameter(torch.zeros((heads, self.d_k)), requires_grad=True) # <5>

# %% ../../nbs/transformers.relative_mha.ipynb 13
@patch
def get_scores(self: RelativeMultiHeadAttention, query:torch.Tensor, key:torch.Tensor): # <1>
    "Get relative attention scores."

    key_pos_emb = self.key_pos_embeddings[self.P - key.shape[0]:self.P + query.shape[0]] # <2>
    key_pos_bias = self.key_pos_bias[self.P - key.shape[0]:self.P + query.shape[0]] # <3>
    query_pos_bias = self.query_pos_bias[None, None, :, :] # <4>
    ac = torch.einsum('ibhd,jbhd->ijbh', query + query_pos_bias, key) # <5>
    b = torch.einsum('ibhd,jhd->ijbh', query, key_pos_emb) # <6>
    d = key_pos_bias[None, :, None, :] # <7>
    bd = shift_right(b + d) # <8>
    bd = bd[:, -key.shape[0]:] # <9>

    # Return the sum
    return ac + bd
